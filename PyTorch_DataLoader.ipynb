{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvBAU6q5JGqEw7PvO9J5Mc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YdoUcare-qm/LearningDeep/blob/main/PyTorch_DataLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "O1CrGalVcGS-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"souravsv/sample-csv-datasets\",\"loan_prediction.csv\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2kNCfjzHwwj",
        "outputId": "50c2297b-9487-4b3b-839d-3a24be499ca6"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/souravsv/sample-csv-datasets/versions/1/loan_prediction.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ],
      "metadata": {
        "id": "90qb8C-yLhxZ"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xy=pd.read_csv(\"/root/.cache/kagglehub/datasets/souravsv/sample-csv-datasets/versions/1/loan_prediction.csv\")\n",
        "cols_to_drop=['Loan_ID','Dependents']\n",
        "xy=xy.drop(cols_to_drop,axis=1)\n",
        "\n",
        "xy=xy.dropna(axis=0)\n",
        "\n",
        "le=LabelEncoder()\n",
        "for cat in ['Gender','Married','Education',\n",
        "            'Self_Employed','Credit_History','Property_Area','Loan_Status']:\n",
        "  xy[cat]=le.fit_transform(xy[cat])\n",
        "\n",
        "xy=xy.values\n",
        "ds=torch.tensor(xy)\n",
        "X=ds[:,:10]\n",
        "y=ds[:,10]"
      ],
      "metadata": {
        "id": "Xj6eKWD1laLN"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
        "y_train=y_train.reshape(-1,1)\n",
        "y_test=y_test.reshape(-1,1)"
      ],
      "metadata": {
        "id": "mULKca-Qll0p"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoanDataset(Dataset):\n",
        "  def __init__(self,x,y):\n",
        "    self.x=x\n",
        "    self.y=y\n",
        "  def __getitem__(self,index):\n",
        "    return self.x[index],self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]"
      ],
      "metadata": {
        "id": "apQEJltjccWA"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train=LoanDataset(X_train,y_train)\n",
        "ds_test=LoanDataset(X_test,y_test)\n",
        "ds_train[0],len(ds_train),ds_test[0],len(ds_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC8YyZJvUK5P",
        "outputId": "f998dd1a-7e09-432b-ea20-ec814b6c40ae"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tensor([1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 5.5000e+03, 1.2600e+03,\n",
              "          1.7000e+02, 3.6000e+02, 1.0000e+00, 0.0000e+00], dtype=torch.float64),\n",
              "  tensor([1.], dtype=torch.float64)),\n",
              " 392,\n",
              " (tensor([0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.9630e+03, 0.0000e+00,\n",
              "          5.3000e+01, 3.6000e+02, 1.0000e+00, 1.0000e+00], dtype=torch.float64),\n",
              "  tensor([1.], dtype=torch.float64)),\n",
              " 98)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=DataLoader(ds_train,batch_size=4,shuffle=True)\n",
        "test_loader=DataLoader(ds_test,batch_size=4,shuffle=True)"
      ],
      "metadata": {
        "id": "hLFZ1L3QkgYN"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "DdMtyOB2fZeT"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self,input_features):\n",
        "    super(LogisticRegression,self).__init__()\n",
        "    self.linear=nn.Linear(input_features,1,dtype=torch.float64)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return torch.sigmoid(self.linear(x))\n"
      ],
      "metadata": {
        "id": "RRSwbwUbe3aI"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=300\n",
        "n_iter=math.ceil(len(ds_train)/4)\n",
        "lr=0.01"
      ],
      "metadata": {
        "id": "7I1mj1Jbijwd"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=LogisticRegression(10)\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=lr)\n",
        "loss=nn.BCELoss()"
      ],
      "metadata": {
        "id": "FLu7lpkEiY7F"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  eloss=0\n",
        "  for i,(input,label) in enumerate(train_loader):\n",
        "    y_pred=model(input)\n",
        "    l=loss(y_pred,label)\n",
        "    with torch.no_grad():\n",
        "      eloss+=l\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    # print(f'Iteration:{i}/{n_iter}')\n",
        "  print(f'Epoch:{epoch+1}/{epochs} Loss:{(eloss/97):.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ZENOP0_inyRC",
        "outputId": "dfab46a0-f0b0-46a4-d491-df6afc474c40"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1/300 Loss:35.040791\n",
            "Epoch:2/300 Loss:32.216495\n",
            "Epoch:3/300 Loss:32.216495\n",
            "Epoch:4/300 Loss:32.216495\n",
            "Epoch:5/300 Loss:32.216495\n",
            "Epoch:6/300 Loss:32.216495\n",
            "Epoch:7/300 Loss:32.216495\n",
            "Epoch:8/300 Loss:32.216495\n",
            "Epoch:9/300 Loss:32.216495\n",
            "Epoch:10/300 Loss:32.216495\n",
            "Epoch:11/300 Loss:32.216495\n",
            "Epoch:12/300 Loss:32.216495\n",
            "Epoch:13/300 Loss:32.216495\n",
            "Epoch:14/300 Loss:32.216495\n",
            "Epoch:15/300 Loss:32.216495\n",
            "Epoch:16/300 Loss:32.216495\n",
            "Epoch:17/300 Loss:32.216495\n",
            "Epoch:18/300 Loss:32.216495\n",
            "Epoch:19/300 Loss:32.216495\n",
            "Epoch:20/300 Loss:32.216495\n",
            "Epoch:21/300 Loss:32.216495\n",
            "Epoch:22/300 Loss:32.216495\n",
            "Epoch:23/300 Loss:32.216495\n",
            "Epoch:24/300 Loss:32.216495\n",
            "Epoch:25/300 Loss:32.216495\n",
            "Epoch:26/300 Loss:32.216495\n",
            "Epoch:27/300 Loss:32.216495\n",
            "Epoch:28/300 Loss:32.216495\n",
            "Epoch:29/300 Loss:32.216495\n",
            "Epoch:30/300 Loss:32.216495\n",
            "Epoch:31/300 Loss:32.216495\n",
            "Epoch:32/300 Loss:32.216495\n",
            "Epoch:33/300 Loss:32.216495\n",
            "Epoch:34/300 Loss:32.216495\n",
            "Epoch:35/300 Loss:32.216495\n",
            "Epoch:36/300 Loss:32.216495\n",
            "Epoch:37/300 Loss:32.216495\n",
            "Epoch:38/300 Loss:32.216495\n",
            "Epoch:39/300 Loss:32.216495\n",
            "Epoch:40/300 Loss:32.216495\n",
            "Epoch:41/300 Loss:32.216495\n",
            "Epoch:42/300 Loss:32.216495\n",
            "Epoch:43/300 Loss:32.216495\n",
            "Epoch:44/300 Loss:32.216495\n",
            "Epoch:45/300 Loss:32.216495\n",
            "Epoch:46/300 Loss:32.216495\n",
            "Epoch:47/300 Loss:32.216495\n",
            "Epoch:48/300 Loss:32.216495\n",
            "Epoch:49/300 Loss:32.216495\n",
            "Epoch:50/300 Loss:32.216495\n",
            "Epoch:51/300 Loss:32.216495\n",
            "Epoch:52/300 Loss:32.216495\n",
            "Epoch:53/300 Loss:32.216495\n",
            "Epoch:54/300 Loss:32.216495\n",
            "Epoch:55/300 Loss:32.216495\n",
            "Epoch:56/300 Loss:32.216495\n",
            "Epoch:57/300 Loss:32.216495\n",
            "Epoch:58/300 Loss:32.216495\n",
            "Epoch:59/300 Loss:32.216495\n",
            "Epoch:60/300 Loss:32.216495\n",
            "Epoch:61/300 Loss:32.216495\n",
            "Epoch:62/300 Loss:32.216495\n",
            "Epoch:63/300 Loss:32.216495\n",
            "Epoch:64/300 Loss:32.216495\n",
            "Epoch:65/300 Loss:32.216495\n",
            "Epoch:66/300 Loss:32.216495\n",
            "Epoch:67/300 Loss:32.216495\n",
            "Epoch:68/300 Loss:32.216495\n",
            "Epoch:69/300 Loss:32.216495\n",
            "Epoch:70/300 Loss:32.216495\n",
            "Epoch:71/300 Loss:32.216495\n",
            "Epoch:72/300 Loss:32.216495\n",
            "Epoch:73/300 Loss:32.216495\n",
            "Epoch:74/300 Loss:32.216495\n",
            "Epoch:75/300 Loss:32.216495\n",
            "Epoch:76/300 Loss:32.216495\n",
            "Epoch:77/300 Loss:32.216495\n",
            "Epoch:78/300 Loss:32.216495\n",
            "Epoch:79/300 Loss:32.216495\n",
            "Epoch:80/300 Loss:32.216495\n",
            "Epoch:81/300 Loss:32.216495\n",
            "Epoch:82/300 Loss:32.216495\n",
            "Epoch:83/300 Loss:32.216495\n",
            "Epoch:84/300 Loss:32.216495\n",
            "Epoch:85/300 Loss:32.216495\n",
            "Epoch:86/300 Loss:32.216495\n",
            "Epoch:87/300 Loss:32.216495\n",
            "Epoch:88/300 Loss:32.216495\n",
            "Epoch:89/300 Loss:32.216495\n",
            "Epoch:90/300 Loss:32.216495\n",
            "Epoch:91/300 Loss:32.216495\n",
            "Epoch:92/300 Loss:32.216495\n",
            "Epoch:93/300 Loss:32.216495\n",
            "Epoch:94/300 Loss:32.216495\n",
            "Epoch:95/300 Loss:32.216495\n",
            "Epoch:96/300 Loss:32.216495\n",
            "Epoch:97/300 Loss:32.216495\n",
            "Epoch:98/300 Loss:32.216495\n",
            "Epoch:99/300 Loss:32.216495\n",
            "Epoch:100/300 Loss:32.216495\n",
            "Epoch:101/300 Loss:32.216495\n",
            "Epoch:102/300 Loss:32.216495\n",
            "Epoch:103/300 Loss:32.216495\n",
            "Epoch:104/300 Loss:32.216495\n",
            "Epoch:105/300 Loss:32.216495\n",
            "Epoch:106/300 Loss:32.216495\n",
            "Epoch:107/300 Loss:32.216495\n",
            "Epoch:108/300 Loss:32.216495\n",
            "Epoch:109/300 Loss:32.216495\n",
            "Epoch:110/300 Loss:32.216495\n",
            "Epoch:111/300 Loss:32.216495\n",
            "Epoch:112/300 Loss:32.216495\n",
            "Epoch:113/300 Loss:32.216495\n",
            "Epoch:114/300 Loss:32.216495\n",
            "Epoch:115/300 Loss:32.216495\n",
            "Epoch:116/300 Loss:32.216495\n",
            "Epoch:117/300 Loss:32.216495\n",
            "Epoch:118/300 Loss:32.216495\n",
            "Epoch:119/300 Loss:32.216495\n",
            "Epoch:120/300 Loss:32.216495\n",
            "Epoch:121/300 Loss:32.216495\n",
            "Epoch:122/300 Loss:32.216495\n",
            "Epoch:123/300 Loss:32.216495\n",
            "Epoch:124/300 Loss:32.216495\n",
            "Epoch:125/300 Loss:32.216495\n",
            "Epoch:126/300 Loss:32.216495\n",
            "Epoch:127/300 Loss:32.216495\n",
            "Epoch:128/300 Loss:32.216495\n",
            "Epoch:129/300 Loss:32.216495\n",
            "Epoch:130/300 Loss:32.216495\n",
            "Epoch:131/300 Loss:32.216495\n",
            "Epoch:132/300 Loss:32.216495\n",
            "Epoch:133/300 Loss:32.216495\n",
            "Epoch:134/300 Loss:32.216495\n",
            "Epoch:135/300 Loss:32.216495\n",
            "Epoch:136/300 Loss:32.216495\n",
            "Epoch:137/300 Loss:32.216495\n",
            "Epoch:138/300 Loss:32.216495\n",
            "Epoch:139/300 Loss:32.216495\n",
            "Epoch:140/300 Loss:32.216495\n",
            "Epoch:141/300 Loss:32.216495\n",
            "Epoch:142/300 Loss:32.216495\n",
            "Epoch:143/300 Loss:32.216495\n",
            "Epoch:144/300 Loss:32.216495\n",
            "Epoch:145/300 Loss:32.216495\n",
            "Epoch:146/300 Loss:32.216495\n",
            "Epoch:147/300 Loss:32.216495\n",
            "Epoch:148/300 Loss:32.216495\n",
            "Epoch:149/300 Loss:32.216495\n",
            "Epoch:150/300 Loss:32.216495\n",
            "Epoch:151/300 Loss:32.216495\n",
            "Epoch:152/300 Loss:32.216495\n",
            "Epoch:153/300 Loss:32.216495\n",
            "Epoch:154/300 Loss:32.216495\n",
            "Epoch:155/300 Loss:32.216495\n",
            "Epoch:156/300 Loss:32.216495\n",
            "Epoch:157/300 Loss:32.216495\n",
            "Epoch:158/300 Loss:32.216495\n",
            "Epoch:159/300 Loss:32.216495\n",
            "Epoch:160/300 Loss:32.216495\n",
            "Epoch:161/300 Loss:32.216495\n",
            "Epoch:162/300 Loss:32.216495\n",
            "Epoch:163/300 Loss:32.216495\n",
            "Epoch:164/300 Loss:32.216495\n",
            "Epoch:165/300 Loss:32.216495\n",
            "Epoch:166/300 Loss:32.216495\n",
            "Epoch:167/300 Loss:32.216495\n",
            "Epoch:168/300 Loss:32.216495\n",
            "Epoch:169/300 Loss:32.216495\n",
            "Epoch:170/300 Loss:32.216495\n",
            "Epoch:171/300 Loss:32.216495\n",
            "Epoch:172/300 Loss:32.216495\n",
            "Epoch:173/300 Loss:32.216495\n",
            "Epoch:174/300 Loss:32.216495\n",
            "Epoch:175/300 Loss:32.216495\n",
            "Epoch:176/300 Loss:32.216495\n",
            "Epoch:177/300 Loss:32.216495\n",
            "Epoch:178/300 Loss:32.216495\n",
            "Epoch:179/300 Loss:32.216495\n",
            "Epoch:180/300 Loss:32.216495\n",
            "Epoch:181/300 Loss:32.216495\n",
            "Epoch:182/300 Loss:32.216495\n",
            "Epoch:183/300 Loss:32.216495\n",
            "Epoch:184/300 Loss:32.216495\n",
            "Epoch:185/300 Loss:32.216495\n",
            "Epoch:186/300 Loss:32.216495\n",
            "Epoch:187/300 Loss:32.216495\n",
            "Epoch:188/300 Loss:32.216495\n",
            "Epoch:189/300 Loss:32.216495\n",
            "Epoch:190/300 Loss:32.216495\n",
            "Epoch:191/300 Loss:32.216495\n",
            "Epoch:192/300 Loss:32.216495\n",
            "Epoch:193/300 Loss:32.216495\n",
            "Epoch:194/300 Loss:32.216495\n",
            "Epoch:195/300 Loss:32.216495\n",
            "Epoch:196/300 Loss:32.216495\n",
            "Epoch:197/300 Loss:32.216495\n",
            "Epoch:198/300 Loss:32.216495\n",
            "Epoch:199/300 Loss:32.216495\n",
            "Epoch:200/300 Loss:32.216495\n",
            "Epoch:201/300 Loss:32.216495\n",
            "Epoch:202/300 Loss:32.216495\n",
            "Epoch:203/300 Loss:32.216495\n",
            "Epoch:204/300 Loss:32.216495\n",
            "Epoch:205/300 Loss:32.216495\n",
            "Epoch:206/300 Loss:32.216495\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-181-bfbb305029bf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0meloss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# print(f'Iteration:{i}/{n_iter}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Optimizer.step#{self.__class__.__name__}.step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m                 \u001b[0;31m# call optimizer step pre hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                 for pre_hook in chain(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[0;31m# Use positional-only argument to avoid naming collision with aten ops arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;31m# When any inputs are FakeScriptObject, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,precision_score"
      ],
      "metadata": {
        "id": "T537C5byxF-y"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model(X_train)\n",
        "y_pred=y_pred.detach().numpy()\n",
        "y_pred=(y_pred>0.5).astype(np.int32)\n",
        "acc=accuracy_score(y_pred,y_train)\n",
        "prc=precision_score(y_pred,y_train)\n",
        "print(f'Accuracy: {acc} Precision: {prc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gXLDPRJxOAU",
        "outputId": "7aa7e02f-3ce7-4c4c-c7a7-7143ce5e9d8e"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6811224489795918 Precision: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model(X_test)\n",
        "y_pred=y_pred.detach().numpy()\n",
        "y_pred=(y_pred>0.5).astype(np.int32)\n",
        "acc=accuracy_score(y_pred,y_test)\n",
        "prc=precision_score(y_pred,y_test)\n",
        "print(f'Accuracy: {acc} Precision: {prc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98tBwOQNx1wA",
        "outputId": "dec542c8-a392-4b65-fe15-64276f957e54"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7040816326530612 Precision: 1.0\n"
          ]
        }
      ]
    }
  ]
}