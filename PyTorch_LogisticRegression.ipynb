{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNz+wgUdRFl7jdQCqA/5+Ri",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YdoUcare-qm/LearningDeep/blob/main/PyTorch_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "yLlcSHxCb2mX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import sklearn.datasets as datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds=datasets.load_breast_cancer()\n",
        "X,y=ds.data,ds.target\n",
        "X.shape,y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB_UjnXHgH7F",
        "outputId": "54d06984-aa32-4307-a4cb-e70aa4f0e707"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((569, 30), (569,))"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(ds),type(X),type(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FETf-emj9_jM",
        "outputId": "450234a0-a55a-498d-bd86-4ff890c98bb7"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(sklearn.utils._bunch.Bunch, numpy.ndarray, numpy.ndarray)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self,n_input_features):\n",
        "    super(LogisticRegression,self).__init__()\n",
        "    self.lin=nn.Linear(n_input_features,1)\n",
        "  def forward(self,x):\n",
        "    y_pred=torch.sigmoid(self.lin(x))\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "-1Ot-oK_--X7"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "sc=StandardScaler()\n",
        "X_train=sc.fit_transform(X_train)\n",
        "X_test=sc.fit_transform(X_test)\n",
        "\n",
        "X_train=torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test=torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train=torch.from_numpy(y_train.astype(np.float32).reshape(-1,1))\n",
        "y_test=torch.from_numpy(y_test.astype(np.float32).reshape(-1,1))"
      ],
      "metadata": {
        "id": "t0C3w9sFBLgB"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,precision_score"
      ],
      "metadata": {
        "id": "cAhhURBWJeFw"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=LogisticRegression(X_train.shape[1])\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "criterion=nn.BCELoss()"
      ],
      "metadata": {
        "id": "THGIlgf0_vNl"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iter=3000\n",
        "for epoch in range(n_iter):\n",
        "  y_pred=model(X_train)\n",
        "  loss=criterion(y_pred,y_train)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  if epoch%10==0:\n",
        "    print(f'Epoch: {epoch} Training Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ywIZgNuYDPwy",
        "outputId": "1624b0c8-8ef6-434e-ac00-ef7f01058bdc"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Training Loss: 0.8721\n",
            "Epoch: 10 Training Loss: 0.6553\n",
            "Epoch: 20 Training Loss: 0.5339\n",
            "Epoch: 30 Training Loss: 0.4585\n",
            "Epoch: 40 Training Loss: 0.4071\n",
            "Epoch: 50 Training Loss: 0.3696\n",
            "Epoch: 60 Training Loss: 0.3410\n",
            "Epoch: 70 Training Loss: 0.3181\n",
            "Epoch: 80 Training Loss: 0.2994\n",
            "Epoch: 90 Training Loss: 0.2838\n",
            "Epoch: 100 Training Loss: 0.2704\n",
            "Epoch: 110 Training Loss: 0.2588\n",
            "Epoch: 120 Training Loss: 0.2486\n",
            "Epoch: 130 Training Loss: 0.2396\n",
            "Epoch: 140 Training Loss: 0.2315\n",
            "Epoch: 150 Training Loss: 0.2242\n",
            "Epoch: 160 Training Loss: 0.2175\n",
            "Epoch: 170 Training Loss: 0.2115\n",
            "Epoch: 180 Training Loss: 0.2059\n",
            "Epoch: 190 Training Loss: 0.2007\n",
            "Epoch: 200 Training Loss: 0.1960\n",
            "Epoch: 210 Training Loss: 0.1915\n",
            "Epoch: 220 Training Loss: 0.1874\n",
            "Epoch: 230 Training Loss: 0.1836\n",
            "Epoch: 240 Training Loss: 0.1800\n",
            "Epoch: 250 Training Loss: 0.1766\n",
            "Epoch: 260 Training Loss: 0.1734\n",
            "Epoch: 270 Training Loss: 0.1704\n",
            "Epoch: 280 Training Loss: 0.1675\n",
            "Epoch: 290 Training Loss: 0.1649\n",
            "Epoch: 300 Training Loss: 0.1623\n",
            "Epoch: 310 Training Loss: 0.1599\n",
            "Epoch: 320 Training Loss: 0.1576\n",
            "Epoch: 330 Training Loss: 0.1554\n",
            "Epoch: 340 Training Loss: 0.1534\n",
            "Epoch: 350 Training Loss: 0.1514\n",
            "Epoch: 360 Training Loss: 0.1495\n",
            "Epoch: 370 Training Loss: 0.1477\n",
            "Epoch: 380 Training Loss: 0.1459\n",
            "Epoch: 390 Training Loss: 0.1443\n",
            "Epoch: 400 Training Loss: 0.1427\n",
            "Epoch: 410 Training Loss: 0.1411\n",
            "Epoch: 420 Training Loss: 0.1397\n",
            "Epoch: 430 Training Loss: 0.1383\n",
            "Epoch: 440 Training Loss: 0.1369\n",
            "Epoch: 450 Training Loss: 0.1356\n",
            "Epoch: 460 Training Loss: 0.1343\n",
            "Epoch: 470 Training Loss: 0.1331\n",
            "Epoch: 480 Training Loss: 0.1319\n",
            "Epoch: 490 Training Loss: 0.1308\n",
            "Epoch: 500 Training Loss: 0.1297\n",
            "Epoch: 510 Training Loss: 0.1286\n",
            "Epoch: 520 Training Loss: 0.1276\n",
            "Epoch: 530 Training Loss: 0.1266\n",
            "Epoch: 540 Training Loss: 0.1256\n",
            "Epoch: 550 Training Loss: 0.1247\n",
            "Epoch: 560 Training Loss: 0.1237\n",
            "Epoch: 570 Training Loss: 0.1228\n",
            "Epoch: 580 Training Loss: 0.1220\n",
            "Epoch: 590 Training Loss: 0.1211\n",
            "Epoch: 600 Training Loss: 0.1203\n",
            "Epoch: 610 Training Loss: 0.1195\n",
            "Epoch: 620 Training Loss: 0.1188\n",
            "Epoch: 630 Training Loss: 0.1180\n",
            "Epoch: 640 Training Loss: 0.1173\n",
            "Epoch: 650 Training Loss: 0.1166\n",
            "Epoch: 660 Training Loss: 0.1159\n",
            "Epoch: 670 Training Loss: 0.1152\n",
            "Epoch: 680 Training Loss: 0.1145\n",
            "Epoch: 690 Training Loss: 0.1139\n",
            "Epoch: 700 Training Loss: 0.1132\n",
            "Epoch: 710 Training Loss: 0.1126\n",
            "Epoch: 720 Training Loss: 0.1120\n",
            "Epoch: 730 Training Loss: 0.1114\n",
            "Epoch: 740 Training Loss: 0.1108\n",
            "Epoch: 750 Training Loss: 0.1103\n",
            "Epoch: 760 Training Loss: 0.1097\n",
            "Epoch: 770 Training Loss: 0.1092\n",
            "Epoch: 780 Training Loss: 0.1086\n",
            "Epoch: 790 Training Loss: 0.1081\n",
            "Epoch: 800 Training Loss: 0.1076\n",
            "Epoch: 810 Training Loss: 0.1071\n",
            "Epoch: 820 Training Loss: 0.1066\n",
            "Epoch: 830 Training Loss: 0.1061\n",
            "Epoch: 840 Training Loss: 0.1057\n",
            "Epoch: 850 Training Loss: 0.1052\n",
            "Epoch: 860 Training Loss: 0.1047\n",
            "Epoch: 870 Training Loss: 0.1043\n",
            "Epoch: 880 Training Loss: 0.1039\n",
            "Epoch: 890 Training Loss: 0.1034\n",
            "Epoch: 900 Training Loss: 0.1030\n",
            "Epoch: 910 Training Loss: 0.1026\n",
            "Epoch: 920 Training Loss: 0.1022\n",
            "Epoch: 930 Training Loss: 0.1018\n",
            "Epoch: 940 Training Loss: 0.1014\n",
            "Epoch: 950 Training Loss: 0.1010\n",
            "Epoch: 960 Training Loss: 0.1006\n",
            "Epoch: 970 Training Loss: 0.1003\n",
            "Epoch: 980 Training Loss: 0.0999\n",
            "Epoch: 990 Training Loss: 0.0995\n",
            "Epoch: 1000 Training Loss: 0.0992\n",
            "Epoch: 1010 Training Loss: 0.0988\n",
            "Epoch: 1020 Training Loss: 0.0985\n",
            "Epoch: 1030 Training Loss: 0.0982\n",
            "Epoch: 1040 Training Loss: 0.0978\n",
            "Epoch: 1050 Training Loss: 0.0975\n",
            "Epoch: 1060 Training Loss: 0.0972\n",
            "Epoch: 1070 Training Loss: 0.0969\n",
            "Epoch: 1080 Training Loss: 0.0965\n",
            "Epoch: 1090 Training Loss: 0.0962\n",
            "Epoch: 1100 Training Loss: 0.0959\n",
            "Epoch: 1110 Training Loss: 0.0956\n",
            "Epoch: 1120 Training Loss: 0.0953\n",
            "Epoch: 1130 Training Loss: 0.0950\n",
            "Epoch: 1140 Training Loss: 0.0948\n",
            "Epoch: 1150 Training Loss: 0.0945\n",
            "Epoch: 1160 Training Loss: 0.0942\n",
            "Epoch: 1170 Training Loss: 0.0939\n",
            "Epoch: 1180 Training Loss: 0.0937\n",
            "Epoch: 1190 Training Loss: 0.0934\n",
            "Epoch: 1200 Training Loss: 0.0931\n",
            "Epoch: 1210 Training Loss: 0.0929\n",
            "Epoch: 1220 Training Loss: 0.0926\n",
            "Epoch: 1230 Training Loss: 0.0923\n",
            "Epoch: 1240 Training Loss: 0.0921\n",
            "Epoch: 1250 Training Loss: 0.0918\n",
            "Epoch: 1260 Training Loss: 0.0916\n",
            "Epoch: 1270 Training Loss: 0.0914\n",
            "Epoch: 1280 Training Loss: 0.0911\n",
            "Epoch: 1290 Training Loss: 0.0909\n",
            "Epoch: 1300 Training Loss: 0.0907\n",
            "Epoch: 1310 Training Loss: 0.0904\n",
            "Epoch: 1320 Training Loss: 0.0902\n",
            "Epoch: 1330 Training Loss: 0.0900\n",
            "Epoch: 1340 Training Loss: 0.0898\n",
            "Epoch: 1350 Training Loss: 0.0895\n",
            "Epoch: 1360 Training Loss: 0.0893\n",
            "Epoch: 1370 Training Loss: 0.0891\n",
            "Epoch: 1380 Training Loss: 0.0889\n",
            "Epoch: 1390 Training Loss: 0.0887\n",
            "Epoch: 1400 Training Loss: 0.0885\n",
            "Epoch: 1410 Training Loss: 0.0883\n",
            "Epoch: 1420 Training Loss: 0.0881\n",
            "Epoch: 1430 Training Loss: 0.0879\n",
            "Epoch: 1440 Training Loss: 0.0877\n",
            "Epoch: 1450 Training Loss: 0.0875\n",
            "Epoch: 1460 Training Loss: 0.0873\n",
            "Epoch: 1470 Training Loss: 0.0871\n",
            "Epoch: 1480 Training Loss: 0.0869\n",
            "Epoch: 1490 Training Loss: 0.0867\n",
            "Epoch: 1500 Training Loss: 0.0865\n",
            "Epoch: 1510 Training Loss: 0.0864\n",
            "Epoch: 1520 Training Loss: 0.0862\n",
            "Epoch: 1530 Training Loss: 0.0860\n",
            "Epoch: 1540 Training Loss: 0.0858\n",
            "Epoch: 1550 Training Loss: 0.0856\n",
            "Epoch: 1560 Training Loss: 0.0855\n",
            "Epoch: 1570 Training Loss: 0.0853\n",
            "Epoch: 1580 Training Loss: 0.0851\n",
            "Epoch: 1590 Training Loss: 0.0850\n",
            "Epoch: 1600 Training Loss: 0.0848\n",
            "Epoch: 1610 Training Loss: 0.0846\n",
            "Epoch: 1620 Training Loss: 0.0845\n",
            "Epoch: 1630 Training Loss: 0.0843\n",
            "Epoch: 1640 Training Loss: 0.0841\n",
            "Epoch: 1650 Training Loss: 0.0840\n",
            "Epoch: 1660 Training Loss: 0.0838\n",
            "Epoch: 1670 Training Loss: 0.0837\n",
            "Epoch: 1680 Training Loss: 0.0835\n",
            "Epoch: 1690 Training Loss: 0.0834\n",
            "Epoch: 1700 Training Loss: 0.0832\n",
            "Epoch: 1710 Training Loss: 0.0830\n",
            "Epoch: 1720 Training Loss: 0.0829\n",
            "Epoch: 1730 Training Loss: 0.0828\n",
            "Epoch: 1740 Training Loss: 0.0826\n",
            "Epoch: 1750 Training Loss: 0.0825\n",
            "Epoch: 1760 Training Loss: 0.0823\n",
            "Epoch: 1770 Training Loss: 0.0822\n",
            "Epoch: 1780 Training Loss: 0.0820\n",
            "Epoch: 1790 Training Loss: 0.0819\n",
            "Epoch: 1800 Training Loss: 0.0818\n",
            "Epoch: 1810 Training Loss: 0.0816\n",
            "Epoch: 1820 Training Loss: 0.0815\n",
            "Epoch: 1830 Training Loss: 0.0813\n",
            "Epoch: 1840 Training Loss: 0.0812\n",
            "Epoch: 1850 Training Loss: 0.0811\n",
            "Epoch: 1860 Training Loss: 0.0809\n",
            "Epoch: 1870 Training Loss: 0.0808\n",
            "Epoch: 1880 Training Loss: 0.0807\n",
            "Epoch: 1890 Training Loss: 0.0806\n",
            "Epoch: 1900 Training Loss: 0.0804\n",
            "Epoch: 1910 Training Loss: 0.0803\n",
            "Epoch: 1920 Training Loss: 0.0802\n",
            "Epoch: 1930 Training Loss: 0.0801\n",
            "Epoch: 1940 Training Loss: 0.0799\n",
            "Epoch: 1950 Training Loss: 0.0798\n",
            "Epoch: 1960 Training Loss: 0.0797\n",
            "Epoch: 1970 Training Loss: 0.0796\n",
            "Epoch: 1980 Training Loss: 0.0794\n",
            "Epoch: 1990 Training Loss: 0.0793\n",
            "Epoch: 2000 Training Loss: 0.0792\n",
            "Epoch: 2010 Training Loss: 0.0791\n",
            "Epoch: 2020 Training Loss: 0.0790\n",
            "Epoch: 2030 Training Loss: 0.0789\n",
            "Epoch: 2040 Training Loss: 0.0787\n",
            "Epoch: 2050 Training Loss: 0.0786\n",
            "Epoch: 2060 Training Loss: 0.0785\n",
            "Epoch: 2070 Training Loss: 0.0784\n",
            "Epoch: 2080 Training Loss: 0.0783\n",
            "Epoch: 2090 Training Loss: 0.0782\n",
            "Epoch: 2100 Training Loss: 0.0781\n",
            "Epoch: 2110 Training Loss: 0.0780\n",
            "Epoch: 2120 Training Loss: 0.0779\n",
            "Epoch: 2130 Training Loss: 0.0778\n",
            "Epoch: 2140 Training Loss: 0.0777\n",
            "Epoch: 2150 Training Loss: 0.0775\n",
            "Epoch: 2160 Training Loss: 0.0774\n",
            "Epoch: 2170 Training Loss: 0.0773\n",
            "Epoch: 2180 Training Loss: 0.0772\n",
            "Epoch: 2190 Training Loss: 0.0771\n",
            "Epoch: 2200 Training Loss: 0.0770\n",
            "Epoch: 2210 Training Loss: 0.0769\n",
            "Epoch: 2220 Training Loss: 0.0768\n",
            "Epoch: 2230 Training Loss: 0.0767\n",
            "Epoch: 2240 Training Loss: 0.0766\n",
            "Epoch: 2250 Training Loss: 0.0765\n",
            "Epoch: 2260 Training Loss: 0.0764\n",
            "Epoch: 2270 Training Loss: 0.0763\n",
            "Epoch: 2280 Training Loss: 0.0762\n",
            "Epoch: 2290 Training Loss: 0.0762\n",
            "Epoch: 2300 Training Loss: 0.0761\n",
            "Epoch: 2310 Training Loss: 0.0760\n",
            "Epoch: 2320 Training Loss: 0.0759\n",
            "Epoch: 2330 Training Loss: 0.0758\n",
            "Epoch: 2340 Training Loss: 0.0757\n",
            "Epoch: 2350 Training Loss: 0.0756\n",
            "Epoch: 2360 Training Loss: 0.0755\n",
            "Epoch: 2370 Training Loss: 0.0754\n",
            "Epoch: 2380 Training Loss: 0.0753\n",
            "Epoch: 2390 Training Loss: 0.0752\n",
            "Epoch: 2400 Training Loss: 0.0751\n",
            "Epoch: 2410 Training Loss: 0.0751\n",
            "Epoch: 2420 Training Loss: 0.0750\n",
            "Epoch: 2430 Training Loss: 0.0749\n",
            "Epoch: 2440 Training Loss: 0.0748\n",
            "Epoch: 2450 Training Loss: 0.0747\n",
            "Epoch: 2460 Training Loss: 0.0746\n",
            "Epoch: 2470 Training Loss: 0.0745\n",
            "Epoch: 2480 Training Loss: 0.0745\n",
            "Epoch: 2490 Training Loss: 0.0744\n",
            "Epoch: 2500 Training Loss: 0.0743\n",
            "Epoch: 2510 Training Loss: 0.0742\n",
            "Epoch: 2520 Training Loss: 0.0741\n",
            "Epoch: 2530 Training Loss: 0.0740\n",
            "Epoch: 2540 Training Loss: 0.0740\n",
            "Epoch: 2550 Training Loss: 0.0739\n",
            "Epoch: 2560 Training Loss: 0.0738\n",
            "Epoch: 2570 Training Loss: 0.0737\n",
            "Epoch: 2580 Training Loss: 0.0736\n",
            "Epoch: 2590 Training Loss: 0.0736\n",
            "Epoch: 2600 Training Loss: 0.0735\n",
            "Epoch: 2610 Training Loss: 0.0734\n",
            "Epoch: 2620 Training Loss: 0.0733\n",
            "Epoch: 2630 Training Loss: 0.0732\n",
            "Epoch: 2640 Training Loss: 0.0732\n",
            "Epoch: 2650 Training Loss: 0.0731\n",
            "Epoch: 2660 Training Loss: 0.0730\n",
            "Epoch: 2670 Training Loss: 0.0729\n",
            "Epoch: 2680 Training Loss: 0.0729\n",
            "Epoch: 2690 Training Loss: 0.0728\n",
            "Epoch: 2700 Training Loss: 0.0727\n",
            "Epoch: 2710 Training Loss: 0.0726\n",
            "Epoch: 2720 Training Loss: 0.0726\n",
            "Epoch: 2730 Training Loss: 0.0725\n",
            "Epoch: 2740 Training Loss: 0.0724\n",
            "Epoch: 2750 Training Loss: 0.0724\n",
            "Epoch: 2760 Training Loss: 0.0723\n",
            "Epoch: 2770 Training Loss: 0.0722\n",
            "Epoch: 2780 Training Loss: 0.0721\n",
            "Epoch: 2790 Training Loss: 0.0721\n",
            "Epoch: 2800 Training Loss: 0.0720\n",
            "Epoch: 2810 Training Loss: 0.0719\n",
            "Epoch: 2820 Training Loss: 0.0719\n",
            "Epoch: 2830 Training Loss: 0.0718\n",
            "Epoch: 2840 Training Loss: 0.0717\n",
            "Epoch: 2850 Training Loss: 0.0717\n",
            "Epoch: 2860 Training Loss: 0.0716\n",
            "Epoch: 2870 Training Loss: 0.0715\n",
            "Epoch: 2880 Training Loss: 0.0715\n",
            "Epoch: 2890 Training Loss: 0.0714\n",
            "Epoch: 2900 Training Loss: 0.0713\n",
            "Epoch: 2910 Training Loss: 0.0713\n",
            "Epoch: 2920 Training Loss: 0.0712\n",
            "Epoch: 2930 Training Loss: 0.0711\n",
            "Epoch: 2940 Training Loss: 0.0711\n",
            "Epoch: 2950 Training Loss: 0.0710\n",
            "Epoch: 2960 Training Loss: 0.0709\n",
            "Epoch: 2970 Training Loss: 0.0709\n",
            "Epoch: 2980 Training Loss: 0.0708\n",
            "Epoch: 2990 Training Loss: 0.0707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=y_pred.detach().numpy()\n",
        "y_train=y_train.numpy()\n",
        "y_pred=(y_pred>0.5).astype(np.int32)\n",
        "train_accuracy=accuracy_score(y_train,y_pred)\n",
        "train_precision=precision_score(y_train,y_pred)\n",
        "print('-'*10,'Training','-'*10)\n",
        "print(\"Accuracy : \",train_accuracy)\n",
        "print(\"Precision : \",train_precision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVJ06r0uJV-b",
        "outputId": "ee056b8a-1fd7-48c2-e82f-5cc2872da347"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Training ----------\n",
            "Accuracy :  0.9846153846153847\n",
            "Precision :  0.9795918367346939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test=model(X_test)\n",
        "L=criterion(y_test,y_pred_test)\n",
        "y_pred_test=y_pred_test.detach().numpy()\n",
        "y_pred_test=(y_pred_test>0.5).astype(np.int32)\n",
        "accuracy=accuracy_score(y_test,y_pred_test)\n",
        "precision=precision_score(y_test,y_pred_test)\n",
        "print('-'*10,'Testing','-'*10)\n",
        "print(\"Accuracy : \",accuracy)\n",
        "print(\"Precision : \",precision)\n",
        "print(\"Loss : \",L)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7QUbCkkMBUF",
        "outputId": "2dd17c6a-fefb-426c-e402-1fc1c983531f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Testing ----------\n",
            "Accuracy :  0.956140350877193\n",
            "Precision :  0.9565217391304348\n",
            "Loss :  tensor(6.2208, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    }
  ]
}