{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YdoUcare-qm/LearningDeep/blob/main/Pytorch_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Nl3jhzQYpq0e",
        "outputId": "f2a65425-5aae-4ee1-bb25-7c70e3da9ee9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device(\"cuda\")\n",
        "else:\n",
        "  device=torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.ones(6,requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "id": "-8NUSQ3qAX5V",
        "outputId": "19b953e8-6683-4d92-864d-83206527335b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1., 1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=x*2\n",
        "y"
      ],
      "metadata": {
        "id": "aae4GdwvBkm1",
        "outputId": "a0b422bf-7e5b-4195-8ab8-fdf65302e113",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z=y*y*2\n",
        "z=z.mean()\n",
        "z"
      ],
      "metadata": {
        "id": "R5Up86V5Bnxt",
        "outputId": "b871b1ab-b525-4322-d37b-e66c1544fdae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8., grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()"
      ],
      "metadata": {
        "id": "6R-LfX2LCm7l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "id": "Cj9PtfaACqNr",
        "outputId": "1b75bf26-fb38-4923-824e-f85cba6c1b28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.6667, 2.6667, 2.6667, 2.6667, 2.6667, 2.6667])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=x*2\n",
        "y\n",
        "y=y.mean()\n",
        "y.backward()\n",
        "y,x.grad"
      ],
      "metadata": {
        "id": "_ACWeoFNCroT",
        "outputId": "1cbdbd56-0fa4-487f-c241-678e3d2d52ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(2., grad_fn=<MeanBackward0>), tensor([3., 3., 3., 3., 3., 3.]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w=torch.tensor(1.0,requires_grad=True)\n",
        "\n",
        "x=torch.tensor(5.0)\n",
        "y=torch.tensor(10.0)\n",
        "\n",
        "y_hat=w*x\n",
        "loss= (y-y_hat)**2\n",
        "\n",
        "loss.backward()\n",
        "w.grad"
      ],
      "metadata": {
        "id": "DMBsI8uLNAP3",
        "outputId": "50076ea1-f220-4eab-9760-8e614697fcd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-50.)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent Algorithm"
      ],
      "metadata": {
        "id": "NdKKK26FmE7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Manual"
      ],
      "metadata": {
        "id": "LW9vyVveqDez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array([1,2,3,4,5,6,7,8,9,10])\n",
        "y=np.array([5,10,15,20,25,30,35,40,45,50])\n",
        "x,y"
      ],
      "metadata": {
        "id": "PQW2btrGmEP8",
        "outputId": "c857a606-d6b8-4a42-98fd-4aff9e8938e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
              " array([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w=0\n",
        "#forward pass\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "def loss(y_hat):\n",
        "  return 1/len(y_hat) * np.sum((y-y_hat)**2)\n",
        "\n",
        "def gradient(y_hat):\n",
        "  return 1/len(y_hat) * 2 * np.sum(y_hat-y)\n",
        "\n",
        "n_iter=100\n",
        "lr=1e-2\n",
        "for epoch in range(n_iter):\n",
        "  y_pred=forward(x)\n",
        "  l=loss(y_pred)\n",
        "  grad=gradient(y_pred)\n",
        "  w-= lr * grad\n",
        "  print(f'Epoch : {epoch + 1} Training Loss : {l:.3f} grad : {grad} updated weight : {w}')"
      ],
      "metadata": {
        "id": "tr0Chf0DmcIL",
        "outputId": "c9ba66ca-a56f-499b-f6be-3006f0090dae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1 Training Loss : 962.500 grad : -55.0 updated weight : 0.55\n",
            "Epoch : 2 Training Loss : 762.396 grad : -48.95 updated weight : 1.0395\n",
            "Epoch : 3 Training Loss : 603.894 grad : -43.5655 updated weight : 1.475155\n",
            "Epoch : 4 Training Loss : 478.344 grad : -38.773295000000005 updated weight : 1.8628879500000002\n",
            "Epoch : 5 Training Loss : 378.897 grad : -34.508232549999995 updated weight : 2.2079702755\n",
            "Epoch : 6 Training Loss : 300.124 grad : -30.712326969499998 updated weight : 2.515093545195\n",
            "Epoch : 7 Training Loss : 237.728 grad : -27.333971002855005 updated weight : 2.7884332552235502\n",
            "Epoch : 8 Training Loss : 188.305 grad : -24.32723419254095 updated weight : 3.0317055971489597\n",
            "Epoch : 9 Training Loss : 149.156 grad : -21.651238431361445 updated weight : 3.2482179814625742\n",
            "Epoch : 10 Training Loss : 118.146 grad : -19.269602203911685 updated weight : 3.440914003501691\n",
            "Epoch : 11 Training Loss : 93.584 grad : -17.1499459614814 updated weight : 3.6124134631165052\n",
            "Epoch : 12 Training Loss : 74.128 grad : -15.263451905718444 updated weight : 3.7650479821736895\n",
            "Epoch : 13 Training Loss : 58.717 grad : -13.584472196089417 updated weight : 3.9008927041345838\n",
            "Epoch : 14 Training Loss : 46.509 grad : -12.09018025451958 updated weight : 4.02179450667978\n",
            "Epoch : 15 Training Loss : 36.840 grad : -10.760260426522422 updated weight : 4.129397110945004\n",
            "Epoch : 16 Training Loss : 29.181 grad : -9.576631779604952 updated weight : 4.2251634287410536\n",
            "Epoch : 17 Training Loss : 23.114 grad : -8.523202283848411 updated weight : 4.310395451579538\n",
            "Epoch : 18 Training Loss : 18.309 grad : -7.585650032625082 updated weight : 4.3862519519057885\n",
            "Epoch : 19 Training Loss : 14.502 grad : -6.751228529036325 updated weight : 4.453764237196152\n",
            "Epoch : 20 Training Loss : 11.487 grad : -6.008593390842326 updated weight : 4.5138501711045755\n",
            "Epoch : 21 Training Loss : 9.099 grad : -5.347648117849669 updated weight : 4.567326652283072\n",
            "Epoch : 22 Training Loss : 7.207 grad : -4.759406824886203 updated weight : 4.614920720531934\n",
            "Epoch : 23 Training Loss : 5.709 grad : -4.235872074148723 updated weight : 4.657279441273421\n",
            "Epoch : 24 Training Loss : 4.522 grad : -3.7699261459923656 updated weight : 4.694978702733345\n",
            "Epoch : 25 Training Loss : 3.582 grad : -3.355234269933203 updated weight : 4.728531045432677\n",
            "Epoch : 26 Training Loss : 2.837 grad : -2.9861585002405486 updated weight : 4.7583926304350825\n",
            "Epoch : 27 Training Loss : 2.247 grad : -2.657681065214092 updated weight : 4.784969441087224\n",
            "Epoch : 28 Training Loss : 1.780 grad : -2.3653361480405373 updated weight : 4.808622802567629\n",
            "Epoch : 29 Training Loss : 1.410 grad : -2.1051491717560817 updated weight : 4.82967429428519\n",
            "Epoch : 30 Training Loss : 1.117 grad : -1.8735827628629125 updated weight : 4.848410121913819\n",
            "Epoch : 31 Training Loss : 0.885 grad : -1.6674886589479918 updated weight : 4.865085008503299\n",
            "Epoch : 32 Training Loss : 0.701 grad : -1.4840649064637104 updated weight : 4.879925657567936\n",
            "Epoch : 33 Training Loss : 0.555 grad : -1.3208177667527012 updated weight : 4.893133835235463\n",
            "Epoch : 34 Training Loss : 0.440 grad : -1.1755278124099098 updated weight : 4.904889113359562\n",
            "Epoch : 35 Training Loss : 0.348 grad : -1.0462197530448227 updated weight : 4.91535131089001\n",
            "Epoch : 36 Training Loss : 0.276 grad : -0.9311355802098902 updated weight : 4.924662666692109\n",
            "Epoch : 37 Training Loss : 0.219 grad : -0.8287106663868002 updated weight : 4.932949773355977\n",
            "Epoch : 38 Training Loss : 0.173 grad : -0.7375524930842489 updated weight : 4.94032529828682\n",
            "Epoch : 39 Training Loss : 0.137 grad : -0.6564217188449826 updated weight : 4.946889515475269\n",
            "Epoch : 40 Training Loss : 0.109 grad : -0.5842153297720385 updated weight : 4.95273166877299\n",
            "Epoch : 41 Training Loss : 0.086 grad : -0.5199516434971114 updated weight : 4.957931185207961\n",
            "Epoch : 42 Training Loss : 0.068 grad : -0.46275696271242595 updated weight : 4.962558754835086\n",
            "Epoch : 43 Training Loss : 0.054 grad : -0.4118536968140587 updated weight : 4.966677291803226\n",
            "Epoch : 44 Training Loss : 0.043 grad : -0.36654979016451605 updated weight : 4.970342789704871\n",
            "Epoch : 45 Training Loss : 0.034 grad : -0.3262293132464194 updated weight : 4.973605082837335\n",
            "Epoch : 46 Training Loss : 0.027 grad : -0.29034408878931206 updated weight : 4.9765085237252285\n",
            "Epoch : 47 Training Loss : 0.021 grad : -0.25840623902248705 updated weight : 4.979092586115454\n",
            "Epoch : 48 Training Loss : 0.017 grad : -0.22998155273001064 updated weight : 4.981392401642753\n",
            "Epoch : 49 Training Loss : 0.013 grad : -0.20468358192971348 updated weight : 4.983439237462051\n",
            "Epoch : 50 Training Loss : 0.011 grad : -0.18216838791744294 updated weight : 4.985260921341225\n",
            "Epoch : 51 Training Loss : 0.008 grad : -0.16212986524652617 updated weight : 4.98688221999369\n",
            "Epoch : 52 Training Loss : 0.007 grad : -0.14429558006940654 updated weight : 4.988325175794384\n",
            "Epoch : 53 Training Loss : 0.005 grad : -0.12842306626177535 updated weight : 4.989609406457002\n",
            "Epoch : 54 Training Loss : 0.004 grad : -0.11429652897297782 updated weight : 4.990752371746732\n",
            "Epoch : 55 Training Loss : 0.003 grad : -0.1017239107859524 updated weight : 4.9917696108545915\n",
            "Epoch : 56 Training Loss : 0.003 grad : -0.09053428059949482 updated weight : 4.992674953660586\n",
            "Epoch : 57 Training Loss : 0.002 grad : -0.08057550973355276 updated weight : 4.993480708757922\n",
            "Epoch : 58 Training Loss : 0.002 grad : -0.07171220366286235 updated weight : 4.99419783079455\n",
            "Epoch : 59 Training Loss : 0.001 grad : -0.0638238612599471 updated weight : 4.99483606940715\n",
            "Epoch : 60 Training Loss : 0.001 grad : -0.056803236521353995 updated weight : 4.995404101772364\n",
            "Epoch : 61 Training Loss : 0.001 grad : -0.050554880503999304 updated weight : 4.995909650577404\n",
            "Epoch : 62 Training Loss : 0.001 grad : -0.04499384364855974 updated weight : 4.996359589013889\n",
            "Epoch : 63 Training Loss : 0.001 grad : -0.040044520847218656 updated weight : 4.996760034222361\n",
            "Epoch : 64 Training Loss : 0.000 grad : -0.03563962355402861 updated weight : 4.997116430457901\n",
            "Epoch : 65 Training Loss : 0.000 grad : -0.03171926496308419 updated weight : 4.997433623107533\n",
            "Epoch : 66 Training Loss : 0.000 grad : -0.02823014581714034 updated weight : 4.997715924565704\n",
            "Epoch : 67 Training Loss : 0.000 grad : -0.025124829777252167 updated weight : 4.9979671728634765\n",
            "Epoch : 68 Training Loss : 0.000 grad : -0.022361098501758825 updated weight : 4.998190783848494\n",
            "Epoch : 69 Training Loss : 0.000 grad : -0.019901377666568187 updated weight : 4.998389797625159\n",
            "Epoch : 70 Training Loss : 0.000 grad : -0.01771222612324852 updated weight : 4.998566919886391\n",
            "Epoch : 71 Training Loss : 0.000 grad : -0.015763881249694123 updated weight : 4.998724558698888\n",
            "Epoch : 72 Training Loss : 0.000 grad : -0.014029854312229695 updated weight : 4.998864857242011\n",
            "Epoch : 73 Training Loss : 0.000 grad : -0.012486570337880032 updated weight : 4.99898972294539\n",
            "Epoch : 74 Training Loss : 0.000 grad : -0.011113047600713521 updated weight : 4.999100853421397\n",
            "Epoch : 75 Training Loss : 0.000 grad : -0.009890612364633178 updated weight : 4.999199759545044\n",
            "Epoch : 76 Training Loss : 0.000 grad : -0.008802645004519327 updated weight : 4.999287785995089\n",
            "Epoch : 77 Training Loss : 0.000 grad : -0.007834354054018 updated weight : 4.9993661295356295\n",
            "Epoch : 78 Training Loss : 0.000 grad : -0.006972575108075141 updated weight : 4.99943585528671\n",
            "Epoch : 79 Training Loss : 0.000 grad : -0.006205591846190295 updated weight : 4.999497911205172\n",
            "Epoch : 80 Training Loss : 0.000 grad : -0.005522976743112196 updated weight : 4.999553140972603\n",
            "Epoch : 81 Training Loss : 0.000 grad : -0.004915449301367403 updated weight : 4.999602295465617\n",
            "Epoch : 82 Training Loss : 0.000 grad : -0.0043747498782158445 updated weight : 4.9996460429643985\n",
            "Epoch : 83 Training Loss : 0.000 grad : -0.003893527391615237 updated weight : 4.999684978238315\n",
            "Epoch : 84 Training Loss : 0.000 grad : -0.0034652393785394864 updated weight : 4.9997196306321\n",
            "Epoch : 85 Training Loss : 0.000 grad : -0.0030840630468986774 updated weight : 4.999750471262569\n",
            "Epoch : 86 Training Loss : 0.000 grad : -0.002744816111739823 updated weight : 4.999777919423686\n",
            "Epoch : 87 Training Loss : 0.000 grad : -0.002442886339449224 updated weight : 4.999802348287081\n",
            "Epoch : 88 Training Loss : 0.000 grad : -0.0021741688421110794 updated weight : 4.999824089975502\n",
            "Epoch : 89 Training Loss : 0.000 grad : -0.0019350102694781768 updated weight : 4.999843440078196\n",
            "Epoch : 90 Training Loss : 0.000 grad : -0.0017221591398410398 updated weight : 4.999860661669595\n",
            "Epoch : 91 Training Loss : 0.000 grad : -0.0015327216344589445 updated weight : 4.99987598888594\n",
            "Epoch : 92 Training Loss : 0.000 grad : -0.0013641222546638687 updated weight : 4.999889630108486\n",
            "Epoch : 93 Training Loss : 0.000 grad : -0.0012140688066565987 updated weight : 4.999901770796552\n",
            "Epoch : 94 Training Loss : 0.000 grad : -0.001080521237924792 updated weight : 4.999912576008931\n",
            "Epoch : 95 Training Loss : 0.000 grad : -0.0009616639017551165 updated weight : 4.999922192647949\n",
            "Epoch : 96 Training Loss : 0.000 grad : -0.0008558808725609879 updated weight : 4.999930751456675\n",
            "Epoch : 97 Training Loss : 0.000 grad : -0.00076173397657886 updated weight : 4.999938368796441\n",
            "Epoch : 98 Training Loss : 0.000 grad : -0.0006779432391510909 updated weight : 4.999945148228832\n",
            "Epoch : 99 Training Loss : 0.000 grad : -0.0006033694828460057 updated weight : 4.99995118192366\n",
            "Epoch : 100 Training Loss : 0.000 grad : -0.0005369988397356806 updated weight : 4.9999565519120575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### using backward() and requires_grad=True"
      ],
      "metadata": {
        "id": "8rbISZ1evhc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([1,2,3,4,5,6,7,8,9,10],dtype=torch.float32)\n",
        "y=torch.tensor([5,10,15,20,25,30,35,40,45,50],dtype=torch.float32)\n",
        "\n",
        "w= torch.tensor(0.0,requires_grad=True,dtype=torch.float32)\n",
        "x,y,w\n",
        "#forward pass\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "def loss(y_hat):\n",
        "  return 1/len(y_hat) * torch.sum((y-y_hat)**2)\n",
        "\n",
        "n_iter=10\n",
        "lr=1e-2\n",
        "\n",
        "for epoch in range(n_iter):\n",
        "  y_pred=forward(x)\n",
        "\n",
        "  l=loss(y_pred)\n",
        "\n",
        "  l.backward()\n",
        "  with torch.no_grad():\n",
        "    w-=lr*w.grad\n",
        "  w.grad.zero_()\n",
        "\n",
        "  print(f'Epoch : {epoch + 1} Training Loss : {l:.3f} grad : {grad:.5f} updated weight : {w}')"
      ],
      "metadata": {
        "id": "Hv2PP2Gpqt-0",
        "outputId": "951f2a93-3c3b-4cbc-fe26-dd2c50333f63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1 Training Loss : 962.500 grad : -0.00054 updated weight : 3.8499999046325684\n",
            "Epoch : 2 Training Loss : 50.916 grad : -0.00054 updated weight : 4.735499858856201\n",
            "Epoch : 3 Training Loss : 2.693 grad : -0.00054 updated weight : 4.939165115356445\n",
            "Epoch : 4 Training Loss : 0.142 grad : -0.00054 updated weight : 4.986008167266846\n",
            "Epoch : 5 Training Loss : 0.008 grad : -0.00054 updated weight : 4.996781826019287\n",
            "Epoch : 6 Training Loss : 0.000 grad : -0.00054 updated weight : 4.999259948730469\n",
            "Epoch : 7 Training Loss : 0.000 grad : -0.00054 updated weight : 4.9998297691345215\n",
            "Epoch : 8 Training Loss : 0.000 grad : -0.00054 updated weight : 4.999960899353027\n",
            "Epoch : 9 Training Loss : 0.000 grad : -0.00054 updated weight : 4.999990940093994\n",
            "Epoch : 10 Training Loss : 0.000 grad : -0.00054 updated weight : 4.999998092651367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### optim and loss"
      ],
      "metadata": {
        "id": "z3p7DxZjv4Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "3g76tvOyv3X-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([[1,2,3,4,5,6,7,8,9,10]],dtype=torch.float32).reshape(10,1)\n",
        "y=torch.tensor([[5,10,15,20,25,30,35,40,45,50]],dtype=torch.float32).reshape(10,1)\n",
        "w= torch.tensor(0.0,requires_grad=True,dtype=torch.float32)\n",
        "x,y,w\n",
        "#forward pass\n",
        "def forward(x):\n",
        "  return w*x\n",
        "loss=nn.MSELoss()\n",
        "optimizer=optim.SGD([w],lr=0.01)\n",
        "\n",
        "n_iter=10\n",
        "lr=1e-2\n",
        "\n",
        "for epoch in range(n_iter):\n",
        "  #forward pass\n",
        "  y_pred=forward(x)\n",
        "\n",
        "  l=loss(y,y_pred)\n",
        "  l.backward()\n",
        "  g=w.grad\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "\n",
        "  print(f'Epoch : {epoch + 1} Training Loss : {l:.6f} grad : {g:.5f} updated weight : {w}')\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "N7S3P92S7H5u",
        "outputId": "a4125db8-407c-4ebf-f687-eac50253e41f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1 Training Loss : 962.500000 grad : -385.00000 updated weight : 3.8499999046325684\n",
            "Epoch : 2 Training Loss : 50.916260 grad : -88.55001 updated weight : 4.735499858856201\n",
            "Epoch : 3 Training Loss : 2.693472 grad : -20.36651 updated weight : 4.939165115356445\n",
            "Epoch : 4 Training Loss : 0.142484 grad : -4.68429 updated weight : 4.986008167266846\n",
            "Epoch : 5 Training Loss : 0.007537 grad : -1.07737 updated weight : 4.996781826019287\n",
            "Epoch : 6 Training Loss : 0.000399 grad : -0.24780 updated weight : 4.999259948730469\n",
            "Epoch : 7 Training Loss : 0.000021 grad : -0.05698 updated weight : 4.9998297691345215\n",
            "Epoch : 8 Training Loss : 0.000001 grad : -0.01311 updated weight : 4.999960899353027\n",
            "Epoch : 9 Training Loss : 0.000000 grad : -0.00301 updated weight : 4.999990940093994\n",
            "Epoch : 10 Training Loss : 0.000000 grad : -0.00070 updated weight : 4.999998092651367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### using nn module"
      ],
      "metadata": {
        "id": "iYnoPR4UCgMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([[1,2,3,4,5,6,7,8,9,10]],dtype=torch.float32).reshape(10,1)\n",
        "y=torch.tensor([[5,10,15,20,25,30,35,40,45,50]],dtype=torch.float32).reshape(10,1)\n",
        "model=nn.Linear(1,1)\n",
        "print(f'initial parameters:')\n",
        "for name,param in model.named_parameters():\n",
        "  print(name,\" : \",param)\n",
        "loss=nn.MSELoss()\n",
        "optimizer=optim.SGD(model.parameters(),lr=0.01)\n",
        "\n",
        "n_iter=10\n",
        "lr=1e-2\n",
        "\n",
        "for epoch in range(n_iter):\n",
        "  #forward pass\n",
        "  y_pred=model(x)\n",
        "\n",
        "  l=loss(y,y_pred)\n",
        "  l.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  p=list(model.parameters())\n",
        "\n",
        "\n",
        "  print(f'\\nEpoch:{epoch + 1} Training Loss:{l:.3f} updated weight:{p}')\n",
        ""
      ],
      "metadata": {
        "id": "_Oajy21fAwiA",
        "outputId": "e21315c0-d063-47ae-c4fa-bdac1c296f47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial parameters:\n",
            "weight  :  Parameter containing:\n",
            "tensor([[-0.1799]], requires_grad=True)\n",
            "bias  :  Parameter containing:\n",
            "tensor([-0.4784], requires_grad=True)\n",
            "\n",
            "Epoch:1 Training Loss:1060.480 updated weight:[Parameter containing:\n",
            "tensor([[3.8613]], requires_grad=True), Parameter containing:\n",
            "tensor([0.1009], requires_grad=True)]\n",
            "\n",
            "Epoch:2 Training Loss:48.670 updated weight:[Parameter containing:\n",
            "tensor([[4.7270]], requires_grad=True), Parameter containing:\n",
            "tensor([0.2242], requires_grad=True)]\n",
            "\n",
            "Epoch:3 Training Loss:2.247 updated weight:[Parameter containing:\n",
            "tensor([[4.9125]], requires_grad=True), Parameter containing:\n",
            "tensor([0.2497], requires_grad=True)]\n",
            "\n",
            "Epoch:4 Training Loss:0.117 updated weight:[Parameter containing:\n",
            "tensor([[4.9524]], requires_grad=True), Parameter containing:\n",
            "tensor([0.2543], requires_grad=True)]\n",
            "\n",
            "Epoch:5 Training Loss:0.019 updated weight:[Parameter containing:\n",
            "tensor([[4.9611]], requires_grad=True), Parameter containing:\n",
            "tensor([0.2545], requires_grad=True)]\n",
            "\n",
            "Epoch:6 Training Loss:0.014 updated weight:[Parameter containing:\n",
            "tensor([[4.9631]], requires_grad=True), Parameter containing:\n",
            "tensor([0.2537], requires_grad=True)]\n",
            "\n",
            "Epoch:7 Training Loss:0.014 updated weight:[Parameter containing:\n",
            "tensor([[4.9636]], requires_grad=True), Parameter containing:\n",
            "tensor([0.2527], requires_grad=True)]\n",
            "\n",
            "Epoch:8 Training Loss:0.014 updated weight:[Parameter containing:\n",
            "tensor([[4.9638]], requires_grad=True), Parameter containing:\n",
            "tensor([0.2516], requires_grad=True)]\n",
            "\n",
            "Epoch:9 Training Loss:0.014 updated weight:[Parameter containing:\n",
            "tensor([[4.9640]], requires_grad=True), Parameter containing:\n",
            "tensor([0.2506], requires_grad=True)]\n",
            "\n",
            "Epoch:10 Training Loss:0.013 updated weight:[Parameter containing:\n",
            "tensor([[4.9642]], requires_grad=True), Parameter containing:\n",
            "tensor([0.2495], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1aS3DET_GZma"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}